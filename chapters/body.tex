\chapter{Introduction}
\label{ch:introduction}

Dimensionality reduction is one of the key research areas in the field of machine learning. As real world data sets are usually of high dimensionality, such as ..., methods to reduce this dimensionality are sought to alleviate the problems arising when dealing with high dimensional data. 

At present, much of the data being produced, gathered and analyzed is very high dimensional. As a consequence, several challenges arise when analyses are being performed on these vast amounts of high dimensional data.
At present, 

Several problems arise when dealing with high dimensional data:
- distances between pairs of points become less distinguished as dimensionality of the space increases \cite{on_the_surprising_behavior_of_distance_metrics}.
- The volume of a space grows exponentially along with the number of dimensions. This leads to even large data sets appearing sparse in the feature space. Efficient sampling requires a lot of points
- algorithms depend on dimensionality of the data being processed and large computational costs are accrued, especially sampling becomes
- need for more training data, k-means example \cite[p.~263]{understanding_machine_learning}
- algorithm complexity depends on the dimensionality of data being processed, large costs are accrued
- from a more practical perspective, high dimensional data by itself is hard to directly reason about and difficult to visualize in any meaningful sense
- In literature, problems arising from high dimensionality are collectively referred to as the \textit{curse of dimensionality}, as first coined by Richard Bellman in the context of his work on dynamic programming \cite{bellman_curse_of_dimensionality}.
To tackle these problems associated with high dimensional data, reduction of the dimensionality of data is sought through various means of feature selection and feature extraction. Feature selection is ..., whereas feature extraction ... . In this work we focus on purely unsupervised feature extraction through 

Several methods for dimensionality reduction have been developed and attempting to cover a majority of them would be greatly out of scope of the work for this thesis. For comprehensive reviews on other prominent methods the reader is referred to \cite{first_dimensionality_reduction_review, second_dimensionality_reduction_review}.

Fortunately, the so called manifold hypothesis \cite{manifold_hypothesis} often holds for real world data sets. The manifold hypothesis posits that the high dimensional data in question lies near a much lower dimensional manifold. As an example, take This in turn implies that learning  

This, together with the classical Johnson-Lindenstrauss lemma \cite{johnson_lindenstrauss}, which gives bounds to the error 

The manifold assumption and manifold learning. In general, dimensionality reduction aims to preserve as much information of the underlying data as possible. Several different methods for dimensionality reduction have been developed over the years. Arguably the most well known and applied method in practice is Principal Component Analysis (PCA), developed independently by ... \cite{pca_1} and ... \cite{pca_2} for the study of ... . Although very efficient to compute and easy to reason about, PCA however suffers from a few drawbacks, making it not ideal for data visualization purposes. the fact that it only aims to maximize the variance... thus local structure of the data is poorly preserved. On the other hand, methods that aim to preserve local structure of data typically suffer from the curse of dimensionality in several ways, such as ... .

Dimensionality reduction is a tool commonly used in disciplines such as ... for visualization of high dimensional data as 2 or 3 dimensional scatter plots. 

In exploratory data analysis, visualization of the characteristics of data sets plays a key role.

In this work a method for improving the applicability of parametric t-SNE is proposed. In the proposed method a generative model in the form of a Variational Autoencoder is first trained and subsequently used to produce training data for the parametric t-SNE embedding. The benefits of this procedure are severalfold: . Further, each of these benefits are experimentally demonstrated on a variety of data sets, both quantitatively and qualitatively.

The structure of this thesis is as follows. In Chapter~\ref{ch:background_and_related_work} the relevant background and related work to the contribution of this thesis is covered to give context to an audience not previously familiar with variational autoencoders or the t-SNE method for dimensionality reduction. The chapter is organized into two main sections, one that builds up to the theory and recent advances in variational autoencoders from the introduction of neural networks, and the other section presents the development of t-SNE up to its parametrization with neural networks.

The method developed for this thesis is presented in Chapter~\ref{ch:method}. The proposed learning and inference algorithms are presented, together with discussion on extensions and further variations. In addition, relevant implementation details for the application built to produce the results for the experiments chapter are briefly covered.

In Chapter~\ref{ch:experiments} the proposed method is tested experimentally for different properties presented in the previous chapter, as well as compared with existing methods. The experiments chapter includes descriptions for the various data sets being used and definitions for the metrics employed to quantitatively assess the quality of the produced low dimensional embeddings by our method and the methods under comparison.

The final chapter presents discussion and conclusions for the work carried out in this thesis. Additionally, directions for future work are suggested.

\chapter{Background and Related Work}
\label{ch:background_and_related_work}

\section{Variational Autoencoders}
\label{section:variational_autoencoders}

Before moving on to specialized artificial neural network structures and learning objectives.

\subsection{Artificial Neural Networks}
\label{section:artificial_neural_networks}

Artificial neural networks, or as commonly referred to simply as neural networks, are at the core of many recent advances in machine learning. Particularly, deep neural network architectures now hold many state-of-the-art results in fields including computer vision, natural language processing and speech recognition \cite{deep_learning}. More broadly, neural networks are applicable 

Supervised learning, \cite{imagenet}.

Unsupervised and semi-supervised learning, representation learning.

Reinforcement learning, atari \cite{deep_atari}, go \cite{alphago} and later go \textit{tabula rasa} \cite{alphazero}.

The development of ever faster hardware for accelerating the computations needed for deep learning has played a pivotal role in the resurgence of 

A more simplistic view of neural networks would be to consider them as simply very powerful function approximators. The theoretical underpinning for this view is the universal approximation theorem \cite{universal_approximation_theorem}.

\begin{figure}[!htb]
  \centering
  \resizebox{\textwidth}{!}{\input{tikz/nn.tikz}\unskip}
  \caption{Feed-forward neural network with input dimension 3, output dimension 2 and 2 hidden layers of dimension 4 each.}
  \label{fig:ae}
\end{figure}

\subsubsection{Basic Principles}
\newpage \phantom{x}
\newpage

\subsubsection{Learning in Artificial Neural Networks}

History of backprop (citations like/leading to \cite{nature_backprop}). SGD, Momentum, ... second order methods?

\newpage \phantom{x}
\newpage

\subsubsection{Modern Advances and Best Practices}
\newpage \phantom{x}
\newpage

\subsection{Autoencoders}
\label{subsection:autoencoders}

\begin{figure}[!htb]
  \centering
  \resizebox{\textwidth}{!}{\input{tikz/ae.tikz}\unskip}
  \caption{Autoencoder}
  \label{fig:ae}
\end{figure}

\newpage \phantom{x}
\newpage

\subsection{Variational Bayesian Methods}
\label{subsection:variational_inference}

Strategies for computing these intractable integrals 

Further, it is possible to combine automatic differentiation with variational inference to further ease statistical model building and computational efficiency \cite{advi}.

\begin{figure}[!htb]
  \centering
  \resizebox{0.5\textwidth}{!}{\input{tikz/mean_field.tikz}\unskip}
  \caption{Illustration of the mean-field approximation of a full rank Gaussian. Adapted from \cite{variational_inference_review}, Figure 1.}
  \label{fig:mean_field_approximation}
\end{figure}

\subsubsection{Kullback-Leibler Divergence}

The Kullback-Leibler divergence, or KL-divergence for short, is a measure of dissimilarity between two probability distributions.

For continuous probability distributions $p$ and $q$ the Kullback-Leibler divergence is defined as

$$\dkl{p}{q} = \int p(\mathbf{x}) \log \frac{p(\mathbf{x})}{q(\mathbf{x})} d\mathbf{x},$$

and likewise for discrete probability distributions as

$$\dkl{p}{q} = \sum_{i} p(i) \log \frac{p(i)}{q(i)}.$$

The following properties hold for KL-divergence:

\begin{enumerate}
\item $\dkl{p}{q} \geq 0$ (non-negativity)
\item $\dkl{p}{q} = 0 \iff p = q$ (identity of indiscernibles).
\end{enumerate}

One must note however that KL-divergence is asymmetric, i.e. $\dkl{p}{q} \neq \dkl{q}{p}$, and does not obey the triangle inequality, thus cannot readily be used as a metric. Especially asymmetry is important to take into account when interpreting the use of KL-divergence.

\subsubsection{The Evidence Lower Bound}

\newpage

\subsection{Auto Encoding Variational Bayes}

\begin{figure}[!htb]
  \centering
  \resizebox{0.3\textwidth}{!}{\input{tikz/aevb_plate_notation.tikz}\unskip}
  \caption{The VAE graphical model, in plate notation. Adapted from \cite{aevb}, Figure 1.}
  \label{fig:aevb_plate_notation}
\end{figure}

\newcommand{\ptheta}{p_{\mathbf{\theta}}}
\newcommand{\qphi}{q_{\mathbf{\phi}}}
\newcommand{\bxi}{\mathbf{x}^{(i)}}
\newcommand{\bz}{\mathbf{z}}

\subsubsection{Derivation of the VAE Objective Function}

The objective of VAEs is to model the data as well as possible, i.e. maximize the marginal likelihood $\ptheta(\bxi)$ for each data point $i$. By an application of Jensen's inequality we can derive the following lower bound for the marginal likelihood under our latent variable model:

\begin{equation}
\begin{aligned}
\log \ptheta(\bxi) & = \log \int \ptheta(\bxi \vert \bz) p(\bz) d\bz \\
                   & = \log \int \frac{\qphi(\bz \vert \bxi)}{\qphi(\bz \vert \bxi)} \ptheta(\bxi \vert \bz) p(\bz) d\bz \\
                   & = \log \mathbb{E}_{\qphi(\bz \vert \bxi)}\Big[\frac{\ptheta(\bxi \vert \bz) p(\bz)}{\qphi(\bz \vert \bxi)}\Big] \\
                   & \geq \mathbb{E}_{\qphi(\bz \vert \bxi)}\Big[\log \frac{p(\bz)}{\qphi(\bz \vert \bxi)}\Big] + \mathbb{E}_{\qphi(\bz \vert \bxi)}\Big[\log \ptheta(\bxi \vert \bz)\Big] \\
                   & = -\dkl{\qphi(\bz \vert \bxi)}{p(\bz)} + \mathbb{E}_{\qphi(\bz \vert \bxi)}\Big[\log \ptheta(\bxi \vert \bz)\Big].
\end{aligned}
\end{equation}

This bound is better known as the \textit{evidence lower bound}, or ELBO for short. An interpretation for the terms of the last equation is that the objective is to have the posterior $\qphi(\bz \vert \bxi)$ match the prior $p(\bz)$ as accurately as 

In many cases the KL-divergence can be integrated analytically, for instance in the case of two Gaussian distributions. In cases where the KL term cannot be solved analytically, Monte Carlo methods are used to approximate the expectation.

Several modifications to this objective have been proposed, e.g. the $\beta$-VAE \cite{beta_vae, understanding_beta_vae}, $\beta$-TCVAE \cite{beta_tcvae} and InfoVAE \cite{infovae}.

\subsubsection{Obtaining Differentiable Monte Carlo Estimates}

To optimize the parameters $\mathbf{\theta}$ and $\mathbf{\phi}$ of our neural network using backpropagation we require a way to compute the gradient of the expectations of random variables. By introducing a random noise variable as input we are able to obtain samples from the latent distribution by using the \textit{reparameterization trick}.

$$\widetilde{\mathbf{z}} = g_{\mathbf{\phi}}(\mathbf{\epsilon}, \mathbf{z})$$

\subsubsection{Network Structure}

\begin{figure}[!htb]
  \centering
  \resizebox{\textwidth}{!}{\input{tikz/vae.tikz}\unskip}
  \caption{Variational Autoencoder}
  \label{fig:vae}
\end{figure}

\newpage \phantom{x}
\newpage \phantom{x}
\newpage

\section{t-Distributed Stochastic Neighbor Embedding}
\label{section:tsne}

Taking a slight departure from the preceding discussion on neural networks, the theory, and development leading to the acclaimed dimensionality reduction algorithm t-SNE will be presented. At the end of this section, a way to utilize the t-SNE objective to train an embedding function parameterized by a neural network will be shown, thereby connecting this section with the previous in a natural way, as well as covering the final preliminaries to the contribution of this thesis.

\subsection{SNE}
\label{subsection:sne}

The Stochastic Neighbor Embedding (SNE) algorithm \cite{sne}, developed by Hinton and Roweis in 2003, serves as the precursor to the later improved t-SNE algorithm. As such, SNE shares many similarities in its definition with t-SNE, and thus by first developing an understanding of SNE and its pitfalls the motivation behind the t-SNE algorithm become more apparent.

\note{FIXME: As the algorithm's name suggests, the approach SNE takes to perform dimensionality reduction is probabilistic in nature. Without delving directly into the mathematical details, an intuitive overview of the algorithm is that it attempts to embed data points in a lower dimension by preserving probability distributions defining . More concretely, the algorithm proceeds as follows. First, for all data points a probability distribution ... . After these probability distributions have been formed, the actual embedding is performed by attempting to distribute the points in the output space such that similarly defined probability distributions in the output space match those of the input space as closely as possible. The natural way to match two distributions is familiar from the discussion on variational bayes methods, namely the KL-divergence between the input and output space distributions is used as the objective function for the embedding of data points, which can be optimized with various methods, including the previously covered gradient descent.}

To formalize these notions, we begin with how the conditional probability of data point $i$ picking data point $j$ in the input space is defined by the equation

\begin{equation}
p_{j \vert i} = \frac{exp(-\norm{\mathbf{x}_i - \mathbf{x}_j}^2 / 2 \sigma_i^2)}{\sum_{k \neq i} exp(-\norm{\mathbf{x}_i - \mathbf{x}_k}^2 / 2 \sigma_i^2)},
\label{eq:p_j_c_i}
\end{equation}

where the probability can be seen as being derived as if a Gaussian were centered on $\mathbf{x}_i$ and an appropriate normalizing term is used to scale the individual probabilities so that they sum to $1$. The variance (denoted by $\sigma_i$ in the equation) is determined separately for each data point $i$ by forcing the perplexity of the conditional distribution over all other data points (later denoted as $P_i$) to equal some preset value.

Perplexity is an information theoretic measure of how well a distribution predicts its samples. For a given probability distribution, perplexity is defined as the exponentiation of the distribution's entropy $H$, e.g. in the case of our discrete $P_i$:

$$b^{H(P_i)} = b^{-\sum_{j=1}^{n} p_{j \vert i} \log_b p_{j \vert i}},$$

where $b$ is the unit used to measure entropy, typically chosen to be either $2$ or $e$, for bits or nats, respectively. From the definition, we can see that larger variance will be chosen for the Gaussians in areas of the input space where the density of data points is low and smaller variance will be used in high density areas. Effectively, variance is adapted to fit the core assumption that the data points lie on a uniformly sampled manifold within the input space. Perplexity is thus a value proportional to the number of effective local neighbors considered by the algorithm when the embedding is being optimized. In practice, the correct $\sigma_i$ for each $i$ is numerically sought using the bisection method, up to a small error tolerance in the resulting perplexity.

The conditional distribution in the output space is similarly defined with a Gaussian kernel:

$$q_{j \vert i} = \frac{exp(-\norm{\mathbf{y}_i - \mathbf{y}_j}^2)}{\sum_{k \neq i} exp(-\norm{\mathbf{y}_i - \mathbf{y}_k}^2)},$$

where each $\mathbf{y}_i$ represents the point in the embedding for data point $\mathbf{x}_i$. A notable difference is the absence of the variance term $2 \sigma_i^2$, which can be interpreted as having been set to $1$ throughout. This choice of fixed variance can, in turn, be interpreted as smoothing out the manifold of the input space when performing the embedding.

Now that we have defined both the output and input distributions for points, the objective function can be formulated as the sum of KL-divergences between each input-output distribution pair:

\begin{equation}
C_{\text{SNE}} = \sum_i \dkl{P_i}{Q_i} = \sum_i \sum_{j \neq i} p_{j \vert i} \log \frac{p_{j \vert i}}{q_{j \vert i}}.
\label{eq:c_sne}
\end{equation}

To optimize this objective gradient descent is typically used to update the positions of the embedded data points. The partial derivative of the objective w.r.t. a point $i$ in the embedding is given by

\begin{equation}
\frac{\partial C_{\text{SNE}}}{\partial \mathbf{y}_i} = 2 \sum_{j \neq i} (\mathbf{y}_i - \mathbf{y}_j)(p_{j \vert i} - q_{j \vert i} + p_{i \vert j} - q_{i \vert j}).
\label{eq:c_sne_grad}
\end{equation}

This has the intuitive interpretation of a force directed layouting algorithm, where each pair of points is either pushing or pulling on each other, depending on whether they are less or more likely to be neighbors under the output's conditional distribution than the input's.

An important fact to note is that the objective function is in fact non-convex, which implies the unfortunate property that the optimization of the objective function is not guaranteed to terminate at the global optimum due to the potential existence of multiple local minima. With random restarts and other optimization methods, such as gradient descent with momentum, it is possible to attempt to escape poor local minima.

Another important observation to make is how the non-symmetry of the KL-divergence plays a role in penalizing the embedding. \note{Explain how.}

\subsection{t-SNE}
\label{subsection:tsne}

The t-SNE algorithm aims to address two issues of the original SNE algorithm. First, one of the main issues with plain SNE is the so called \textit{crowding problem}, which is addressed by considering a heavier tailed probability distribution function in the output space, namely the Student's t-Distribution. Second, through considering joint distributions, instead of conditional distributions, the non-convex objective function becomes easier to optimize. Together these improvements enable better separation of natural clusters in the output space of the algorithm, as compared to the original SNE method.

The crowding problem is named after the phenomenon where distances in a high dimensional space cannot faithfully be modelled in a lower dimensional space, causing the SNE algorithm to collapse points together in the embedding and thus preventing the formation of natural clusters. One can easily verify that it is not possible in the general case to preserve distances perfectly when performing dimensionality reduction, as a space of $n$ dimensions allows for a maximum of $n+1$ equidistant points. How the crowding problem manifests itself is related to this effect by considering the exponential increase in the volume of a sphere as more dimensions are added. In addition to SNE, the crowding problem is also present in other non-linear dimensionality reduction methods based on preservation of pairwise distances of points, such as Sammon mapping \cite{tsne}.

Unintuitively, as the number of dimensions increase, the concentration of a Gaussian distribution's density shifts from its mean to a shell around its mean \cite[p.~50]{high_dimensional_probability}. When SNE attempts to match probabilities derived from Gaussian kernels in different dimensionalities, the moderately distant points in the high dimensional space (those which are close to the shell of high density concentration) will have a disproportionately large force attracting them to the center of the lower dimensional kernel.

To address the crowding problem the t-SNE algorithm switches the Gaussian kernels of the output space to ones formed with the Student's t-Distribution. The benefit a t-Distribution has over a Gaussian is that it places more of its probability density in its tails. This property of the distribution allows moderate distances in the input space to be modeled further apart in the output space, alleviating the crowding problem by reducing the attractive forces between moderately distant points and thereby facilitating the formation of more distinct clusters in the embedding. In addition, using a t-Distribution has the computational advantage of not having to perform exponentiation when computing probabilities.

Similarly to Symmetric SNE \cite{symmetric_sne}, instead of matching conditional distributions in the input and output spaces, t-SNE forms a joint distribution over all pairs of points in both the input and output space, which it attempts to match. The joint distribution over the input points is formed by using Equation~\ref{eq:p_j_c_i}:

$$p_{ij} = \frac{p_{j \vert i} + p_{i \vert j}}{2 N}$$

$$q_{ij} = \frac{(1 + \norm{\mathbf{y}_i - \mathbf{y}_j}^2)^{-1}}{\sum_{k \neq l} (1 + \norm{\mathbf{y}_k - \mathbf{y}_l}^2)^{-1}}$$

The t-SNE objective function is defined similarly to the SNE objective (cf. Equation~\ref{eq:c_sne}), except the sum is taken over KL-divergences of the joint probabilities:

$$C_{\text{t-SNE}} = \sum_i \dkl{P}{Q} = \sum_{i \neq j} p_{ij} \log \frac{p_{ij}}{q_{ij}}.$$

For which the following gradient can be derived, w.r.t. each $\mathbf{y}_i$:

\begin{equation}
\frac{\partial C_{\text{t-SNE}}}{\partial \mathbf{y}_i} = 4 \sum_{j \neq i} (\mathbf{y}_i - \mathbf{y}_j)(p_{ij} - q_{ij})(1 + \norm{\mathbf{y}_i - \mathbf{y}_j}^2)^{-1}
\label{eq:c_tsne_grad}
\end{equation}

To alleviate the crowding problem, 

Methods to improve the runtime of t-SNE have been subsequently proposed: \cite{accelerated_tsne_1, accelerated_tsne_2}. Use of the fast multipole method \cite{fast_multipole} has not been explored due to no suitable expansion for the t-Distributed forces in the output space exist.

Worth a mention is the Stochastic Triplet Embedding \cite{stochastic_triplet_embedding} and Multiple Maps t-SNE \cite{multiple_maps_tsne} methods developed on top of t-SNE, developed to handle non-metric similarities present in the data.

\newpage \phantom{x}
\newpage \phantom{x}
\newpage

\subsection{Parametric t-SNE}
\label{subsection:parametric_tsne}

A major drawback of most non-linear dimensionality reduction methods is that they do not directly enable out-of-sample extensions.

A natural extension to t-SNE would be to learn a function parameterizing the embedding from input space to output space.

The basic idea behind PTSNE is very straightforward. A feed-forward neural network is used to parameterize a function $f_\mathbf{W}: X \mapsto Y$, which maps points from input space to output space in such a way as to minimize the t-SNE objective. Once the parameters of the mapping are learned, performing dimensionality reduction on any given point is simply done by running it through the neural network.

By application of the chain rule, we can write the gradient w.r.t. the parameters of the neural network as

$$\frac{\partial C_{\text{t-SNE}}}{\partial \mathbf{W}} = \frac{\partial C_{\text{t-SNE}}}{\partial f_{\mathbf{W}}(\mathbf{x}_i)} \frac{\partial f_{\mathbf{W}}(\mathbf{x}_i)}{\partial \mathbf{W}}.$$

As $f_{\mathbf{W}}(\mathbf{x}_i) = \mathbf{y}_i$, Equation~\ref{eq:c_tsne_grad} is used to compute the first term of the product, while standard backpropagation can be applied to the rest of the network.

\newpage

\chapter{Method}
\label{ch:method}

The method proposed in this work involves combining the VAE model with Parametric t-SNE to produce a general framework for improved parametric dimensionality reduction for visualization. In this chapter the 

\section{Learning a Parametric Embedding Using VAE Sampling}

\newpage \phantom{x}
\newpage \phantom{x}
\newpage

\section{Robustness to Sparse and Noisy Data}
\label{section:robustness_to_sparse_and_noisy_data}
\newpage

\section{Sampling from Hidden Layers}
\label{section:sampling_from_hidden_layers}
\newpage

\section{Inference with the Generative Model}
\newpage

\section{Implementation}
\label{section:implementation}

An accompanying implementation of the method has been made available at \url{https://github.com/ahie/vptsne}. The implementation is built on top of Tensorflow \citep{tensorflow} and takes advantage of the Tensorflow Distributions \citep{tensorflow_distributions} library. To take full advantage of the computation graph architecture of Tensorflow and the paralellism provided by GPGPUs, both the t-SNE loss and its gradient computation were implemented as custom CUDA \citep{cuda} kernel operations.

\note{Describe that it also includes fast parallel implementation of NN-Descent \citep{nn_descent}?}

\chapter{Experiments}
\label{ch:experiments}

The code and data used to reproduce all the results presented in this section have been made available at \url{https://github.com/ahie/vptsne-results}.

\section{Data Sets}

To validate the proposed method we studied the properties of the combined model on the four different data sets described below.

\subsection{MNIST}

The MNIST data set \citep{mnist} contains labeled images of handwritten digits. Each image contains a single digit in gray-scale with a resolution of 28x28 pixels. The data set is split into $60000$ training images and $10000$ test images. All networks were trained solely on the training data set, while the remaining test examples were reserved for assessing the capability of the network to generalize to out-of-sample extensions. In the unsupervised setting considered here, the provided labels were only used for visualizing the embeddings and evaluating the embedding quality of out-of-sample extensions via a 1-NN classifier.

\subsection{Fashion-MNIST}

The Fashion-MNIST data set \cite{fashion_mnist} was designed as a drop-in replacement for the MNIST data set and thus contains the same number of grayscale training and test images of equal dimensions. The images themselves consist of 10 different classes of fashion items, such as T-shirts, trousers, sneakers, and handbags. The motivation for considering this data set in addition to MNIST is the fact that it has been shown to be much harder to learn even with supervised computer vision methods.

\subsection{Mass Cytometry}

Cytometry is the measurement of biological characteristics of cells. In mass cytometry, time-of-flight mass spectrometry is used to measure the counts of cellular proteins present within a single cell by tagging proteins with their corresponding antibodies that have been conjugated with specific heavy metals.

As a comparatively new methodological development, mass cytometry enables simultaneous measurement of a considerably greater number of features compared to fluorescence-based flow cytometry. A greater number of features, however, poses new challenges in analyzing the measurement results. To aid the analysis of this high dimensional data it is common to visualize the data as a 2-D or 3-D scatter plot using t-SNE, due to its capability to separate biologically relevant subpopulations of cells in the produced embedding \citep{visne}. However, as a high-throughput method, mass cytometry data sets under analysis can grow to be millions of data points in size, prompting the use of more scalable algorithms. For a more thorough review of mass cytometry and associated analytical challenges, the reader is referred to \cite{mass_cytometry}.

In our experiments, we consider two different mass cytometry data sets. As the first data set, henceforth referred to as Cytometry A, we use the publicly available data of \cite{phenograph}, which has been used to demonstrate the effectiveness of the Phenograph clustering algorithm on mass cytometry data. The data set consists of $81000$ data points of $13$ dimensions corresponding to the normalized counts of cell surface proteins belonging to distinct clusters of differentiation. As a benchmark data set for the Phenograph clustering algorithm, we additionally include the labels produced by Phenograph in our analysis.

The second mass cytometry data set (Cytometry B) contains data gathered from ovarian cancer patients at different phases of treatment. The number of points in the entire data set is $1.4$ million, each of which has $23$ dimensions.

\section{Evaluation Metrics}

To quantitatively evaluate the quality of embeddings two different measures are used in this work. By quality of the embeddings we are interested in how well the local structure of the data is preserved. The choice of the two metrics used in this work follows those used in the original parametric t-SNE paper \cite{parametric_tsne}. A short description for each is given below.

\subsection{k-Nearest Neighbor Classifier}

Given a labeled data set a k-nearest neighbor (k-NN) classifier can be used to roughly assess how well different classes are separated in the output space of an embedding. A k-NN classifier works simply by returning the majority label of a given point's k-nearest neighbors in the training data set. For example in the 1-NN classifier case, the decision boundary can be geometrically visualized as a Voronoi diagram as in Figure~\ref{fig:voronoi}.

Throughout the experiments, the performance of nearest neighbor classifiers in the output space of embeddings will be evaluated. More precisely, the accuracy of a 1-nearest neighbor classifier with the Euclidean metric for determining neighbor distance is used.

For parametric methods, the classifier can be trained on the embedding of training data points and separately tested on the training set. This enables evaluating the ability of the embedding function to generalize to unseen data. On the other hand, for non-parametric methods, the final embedded data points need to be split into train and test sets for the 1-NN classifier.

\begin{figure}[!htb]
  \centering
  \resizebox{0.5\textwidth}{!}{\input{tikz/voronoi.tikz}\unskip}
  \caption{Voronoi diagram illustrating the decision boundaries of 10 different classes of a 1-NN classifier. Black dots represent the training data points, and the different colors correspond to the label of the data point for that region. Any point to be classified would receive the label corresponding to the location in the tesselation it lies on. The distance metric used in this example is the Euclidean distance.}
  \label{fig:voronoi}
\end{figure}

\subsection{Trustworthiness}

Trustworthiness, introduced by Venna et al. \cite{trustworthiness}, is a measure of the degree to which local structure is preserved in the output space relative to the input space after dimensionality reduction is performed. Trustworthiness has been used to compare multiple dimensionality reduction methods for the purposes of visualization of high dimensional data sets such as gene expression data \cite{trustworthiness_gene_expression} and gene interaction graphs \cite{trustworthiness_gene_interaction}.

Computation of trustworthiness is done by comparing how the rank order for each point's $k$ nearest neighbors in the output space match that of the input space. Formally, for a chosen number $k$ of neighbors it is given by the following 

$$T(k) = 1 - \frac{2}{nk (2n - 3k - 1)} \sum^{n}_{i=1} \sum_{j \in \mathcal{N}_{i}^{k}} (r(i, j) - k),$$

where $n$ is the total number of data points under consideration, $\mathcal{N}_{i}^{k}$ is the set of $k$ nearest neighbors in the output space of data point $i$, and $r(i, j)$ is a function giving the rank of node $j$ in the nearest neighbor ordering of node $i$. The possible values for trustworthiness range from 0 to 1, indicating complete loss of local structure and perfect preservation of local structure, respectively.

\section{Network Structure and Parameters}

To parameterize the embedding $f(\mathbf{\hat{x}})$ a feed-forward neural network with layers of hidden units with dimensions $d - 500 - 500 - 2000 - 2$ were used, where $d$ is the input dimensionality and rectifier linear unit (ReLU) activations are applied to the hidden unit outputs to induce nonlinearity to the network. The hidden unit dimensions were chosen to match those of the final combined network used in \citep{parametric_tsne}, although we have substituted the sigmoid activations with ReLU activations. Furthermore, we do not perform stacked restricted Boltzmann machine pretraining on the hidden layer weights as in \citep{parametric_tsne}. However we do benchmark the performance of our network against the original parametric t-SNE implementation. Since we are not considering RBM pretraining and are instead relying on the property of ReLUs reducing the vanishing gradient problem \citep{relu} and better initialization \citep{xavier_initializer}. Running the original implementation shows little benefit in performing this costly initialization. We could instead reuse the trained encoder weights of the VAE as a starting point for the optimization of our embedding network.

For optimization of the neural network \cite{parametric_tsne} use nonlinear conjugate gradient descent, whereas Adam \citep{adam} is used as the optimizer in the results presented here. Updates with Adam are considerably faster to compute than with conjugate gradient descent. Throughout all experiments standard parameters for Adam were chosen, i.e. a learning rate of $0.001$ and the exponential decay rates of the 1st and 2nd moments were set to $0.9$ and $0.999$ respectively.

\textcolor{red}{[Note: Should mention recent work on Adam, i.e. the updated version by \cite{on_the_convergence_of_adam}?]}

We apply batch normalization \citep{batch_normalization} to the hidden layers of the VAE networks, while omitting batch normalization from the embedding network. This is to validate the training procedure on as simple an embedding network as possible. We note that applying batch normalization to the embedding network improves the results marginally.

To further emphasize the universality of our approach we restrict ourselves to use a simple VAE structure, without complicating the model architecture with more recently proposed advances, such as normalizing flows \citep{normalizing_flows} (\textcolor{red}{[Note: could show that better models improve performance in the supplement]}). Throughout all experiments the VAE architecture is fixed to $d - 256 - 128 - 32 - \mu, \log \sigma^2 - 32 - 128 - 256 - d$, where $\mu$ and $\sigma$ parameterize a normal distribution acting as the posterior $q_{\phi}(\mathbf{z} \vert \mathbf{x}^{(i)})$ and the final layer output was used to parameterize a Bernoulli distribution for MNIST as well as Fashion-MNIST, and a normal distribution with a fixed standard deviation of 0.1 for cytometry data. In all experiments, each VAE was trained with a batch size of 1000 for 10000 iterations.

For MNIST and Fashion-MNIST perplexities were set to 30, which is in line with the value used by \cite{parametric_tsne}, whereas for the cytometry data sets a perplexity of 10 was chosen. \textcolor{red}{[Note: Perplexity should really be chosen relative to the batch size. For example training VPTSNE on MNIST with batch size 200 and perplexity 10 gives far better results than with perplexity 30. However, with PTSNE the results are actually worse as the training fails to converge properly, even with small learning rates. An example embedding of MNIST with low perplexity and small batch size has been included in the appendix (figure \ref{fig:low_perp_small_bs}), which also achieves better trustworthiness (0.935) and 1-NN (0.812) than the 600 batch size, 30 perplexity runs. Analysis of why PTSNE fails to converge and VPTSNE does not should be included, since this is a clear advantage of the method we're demonstrating.]}

\section{Learning}
\label{subsection:learning}

In this section we compare the effect of training on VAE reconstructions to training on the original MNIST data set. To quantitatively evaluate the quality of the embeddings produced we employed the trustworthiness metric \citep{trustworthiness} on the MNIST test set. Additionally, we compared the 1-NN classification errors by fitting the classifiers on the produced embeddings of the training set and finding the mean accuracies of the classifiers on the test set.

In figure \ref{fig:mnist_knn_trustworthiness} we have plotted the 1-NN scores and trustworthiness of the embeddings obtained after each iteration of training for two different batch sizes. The runs for each batch size were repeated 20 times, plotting the means and 95\% confidence intervals. Training with VAE reconstructions shows a clear improvement both qualitatively and quantitatively over training on the original data when small training batches are used to approximate the t-SNE loss gradient. Higher trustworthiness, as well as 1-NN scores are obtained consistently and convergence is reached with far fewer iterations. Moreover, the results are in favor of training on reconstructions by exhibiting more stable results during training.

Qualitatively the embeddings trained on the original data remain noisier than the embeddings trained on the reconstructed data points. This can be seen in figure \ref{fig:mnist_comparison}, where the separation of true clusters in the embedding is far less evident, with several classes overlapping and a large number of outlier points for each class. We further compare the qualitative differences of the embeddings produced with the Fashion-MNIST dataset. In figure \ref{fig:fashion_mnist_comparison} similar deficiencies in the embedding can be noted as in the embedding comparisons for the MNIST data set. In particular, the visible clusters of classes are less distinct, as well as the global layout of the classes is considerably worse, e.g. the cluster of footwear related images has been pulled closer to the cluster of upper body garments and the images of bags have been split into two seemingly unrelated clusters.

\begin{figure}[!htb]
  \centering
  \includegraphics[width=\textwidth]{images/learning_curves.png}
  \caption{Plots of 1-NN and trustworthiness scores obtained after a given number of iterations for different batch sizes. The means and 95\% confidence intervals have been plotted from 20 repeated runs for each parameter setting.}
  \label{fig:mnist_knn_trustworthiness}
\end{figure}

\begin{figure}[!htb]
  \centering
  \begin{tabular}{cc}
    \includegraphics[width=0.45\textwidth]{images/mnist_ptsne_embedding.png} &
    \includegraphics[width=0.45\textwidth]{images/mnist_vptsne_embedding.png} \\
    \multicolumn{2}{c}{\includegraphics[width=0.45\textwidth]{images/mnist_legend.png}}
  \end{tabular}
  \caption{Embeddings of the MNIST data set trained with batch size 400.}
  \label{fig:mnist_comparison}
\end{figure}

\begin{figure}[!htb]
  \centering
  \begin{tabular}{cc}
    \includegraphics[width=0.45\textwidth]{images/fashion_mnist_ptsne_embedding.png} &
    \includegraphics[width=0.45\textwidth]{images/fashion_mnist_vptsne_embedding.png} \\
    \multicolumn{2}{c}{\includegraphics[width=0.45\textwidth]{images/fashion_mnist_legend.png}}
  \end{tabular}
  \caption{Embeddings of the Fashion-MNIST data set trained with batch size 400. \textcolor{red}{[Note: could include UMAP embedding of this data set here for comparison. It is remarkably similar and this would reinforce the claim that small batch size training is actually very effective.]}}
  \label{fig:fashion_mnist_comparison}
\end{figure}

\textcolor{red}{??? Plot like Figure~\ref{fig:mnist_knn_trustworthiness} for VPTSNE vs PTSNE with batch size on x to show that VPTSNE does not suffer that much from small batch sizes? This would imply that it can be trained with smaller batch sizes, which means better asymptotic performance. ???} 

\section{Comparisons}
\label{subsection:comparisons}

We now compare the proposed method to other parametric dimensionality reduction methods, namely PCA and VAE. In addition we also consider two non-parametric dimensionality reduction methods, t-SNE and the recently proposed method UMAP \citep{umap}, which had been demonstrated to be able to produce embeddings comparable to t-SNE while being more efficient to optimize in practice.

For the comparisons we will be using the Cytometry A data set. As evaluation criteria, similarly to the previous section, both trustworthiness and 1-NN classification accuracy are used. In addition to comparing the quality of the embeddings, the runtime performance of the different methods are studied. Our goal is to demonstrate that our method produces competitive embeddings as well as has superior scalability.

\subsection{Embedding Quality}
\label{subsubsection:embedding_quality}

The quantitative results for all methods under comparison are available in table \ref{table:comparison} and the corresponding scatter plots of the 2-D embeddings are presented in figure \ref{fig:comparison_figure}. Although VAE produces quantitatively better results for the chosen metrics, the spatial layout of clusters in the resulting embedding have less distinct separation and the global structure of the embedding is constrained by the chosen prior $p(\mathbf{z})$. From a data visualization standpoint these factors make the latent space embeddings qualitatively worse than what the corresponding quantitative metrics would suggest. Unsurprisingly, PCA performs the worst on both quantitative and qualitative results. Our method however is able to reach results on par with those of the chosen non-parametric methods.

\noindent \textcolor{red}{[Note: The effect of choosing a suitable perplexity w.r.t. batch size mentioned in a previous note is highlighted in \ref{fig:comparison_figure}. With large perplexity the clusters in the t-SNE embedding begin to separate more distinctly like with VPTSNE/UMAP, but here the same perplexity is used for both VPTSNE and t-SNE optimization. As perplexity is increased t-SNE optimization becomes considerably slower (for example going from perplexity 10 to 100 increased the total computation time twofold).]}

\noindent \textcolor{red}{[Note: We can also observe that t-SNE (at least with the given perplexity) preserves global structure much more poorly. This can be observed by comparing with the PCA plot, which has three clearly distinct, spatially separated clusters of clusters that are not present in the t-SNE plot. Could consider using some metric to quantify this?]}

\subsection{Scalability}
\label{subsubsection:scalability}

A major practical challenge with nonlinear dimensionality reduction methods are their scalability to large data sets. Typically, to capture the spatial structure of data a distance metric is employed to compute the pairwise distances for the entire data set, after which a projection of the points to a lower dimensional space, equipped with a corresponding metric, is sought that retains the computed distances as accurately as possible. As the number of data points increases, the inherent quadratic complexity of methods relying on such pairwise distance computations quickly renders such methods intractable.

In practice, various approximations and data structures are employed to speed up dimensionality reduction methods. With t-SNE a widely used performance enhancement, independently investigated by \cite{accelerated_tsne_1} and \cite{accelerated_tsne_2}, is to use the Barnes-Hut algorithm \citep{barnes_hut} to approximate the $Q_{ij}$ matrix by averaging the influence of distant points belonging to the same node in the underlying space partitioning data structure. This approximation nevertheless fails to provide significant speedup in the general case depending on the chosen tradeoff between accuracy and speed, as well as the distribution of the points being optimized at each step. Furthermore, its use is limited to obtaining 2 or 3 dimensional embeddings due to its use of the quadtree and octree data structures for each respective dimensionality. On the other hand, optimization of the parametric embedding can be performed with stochastic gradient descent. As we have shown, even with small batch sizes we are able to produce embeddings of competitive quality. The Barnes-Hut approximation can also be used to further speed up the computation of the loss and its gradient, but for small batch sizes the computational overhead of constructing the required data structure can outweigh the cost of directly computing all pairwise influences within the batch. Direct computation has the additional benefit that it is not limited to output dimensionalities for which a space partitioning data structure can be efficiently built.

As previously mentioned, one of the strengths of UMAP compared to t-SNE is the efficiency of its optimization. The key to its efficient optimization is that its gradient can be approximated stochastically via probabilistic edge sampling following the procedure introduced in \citep{largevis}. As both VPTSNE and UMAP take advantage of stochastic gradient descent, the scalability of their optimization is equal up to an implementation dependent constant factor. VPTSNE however maintains the advantage over UMAP that natural out-of-sample extension is made possible through the learned embedding function.

\begin{table}[]
\centering
\caption{Comparison between different dimensionality reduction methods.}
\label{table:comparison}
\begin{tabular}{cccc}
Algorithm   & Trustworthiness ($k=12$)   & 1-NN     & time (ms) \\ \hline \hline
VPTSNE      & $0.9753$                   & $0.9250$ & $60766$   \\ \hline
PTSNE       & $0.9628$                   & $0.8890$ & $8729$    \\ \hline
UMAP        & $0.9688$                   & $0.9297$ & $74716$   \\ \hline
VAE         & $0.9762$                   & $0.9247$ & $52037$   \\ \hline
t-SNE       & $0.9879$                   & $0.9536$ & $1192812$ \\ \hline
PCA         & $0.8557$                   & $0.4886$ & $96$      \\ \hline
\end{tabular}
\end{table}

\begin{figure}[!htb]
  \centering
  \begin{tabular}{cccc}
    VPTSNE &
    \raisebox{-.5\height}{\includegraphics[width=0.25\textwidth]{images/vptsne_levine.png}} &
    \raisebox{-.5\height}{\includegraphics[width=0.25\textwidth]{images/vptsne_levine_zoomed.png}} &
    \raisebox{-.5\height}{\includegraphics[width=0.25\textwidth]{images/vptsne_levine_zoomed_2.png}} \\
    PTSNE &
    \raisebox{-.5\height}{\includegraphics[width=0.25\textwidth]{images/ptsne_levine.png}} &
    \raisebox{-.5\height}{\includegraphics[width=0.25\textwidth]{images/ptsne_levine_zoomed.png}} &
    \raisebox{-.5\height}{\includegraphics[width=0.25\textwidth]{images/ptsne_levine_zoomed_2.png}} \\
    UMAP &
    \raisebox{-.5\height}{\includegraphics[width=0.25\textwidth]{images/umap_levine.png}} &
    \raisebox{-.5\height}{\includegraphics[width=0.25\textwidth]{images/umap_levine_zoomed.png}} &
    \raisebox{-.5\height}{\includegraphics[width=0.25\textwidth]{images/umap_levine_zoomed_2.png}} \\
    VAE &
    \raisebox{-.5\height}{\includegraphics[width=0.25\textwidth]{images/vae_levine.png}} &
    \raisebox{-.5\height}{\includegraphics[width=0.25\textwidth]{images/vae_levine_zoomed.png}} &
    \raisebox{-.5\height}{\includegraphics[width=0.25\textwidth]{images/vae_levine_zoomed_2.png}} \\
    t-SNE &
    \raisebox{-.5\height}{\includegraphics[width=0.25\textwidth]{images/tsne_levine.png}} &
    PCA &
    \raisebox{-.5\height}{\includegraphics[width=0.25\textwidth]{images/pca_levine.png}} \\
  \end{tabular}
  \caption{Cytometry A data set embedded with the methods being compared in section \ref{subsection:comparisons}.}
  \label{fig:comparison_figure}
\end{figure}

\section{Robustness to Sparse Data}

As discussed in section \textcolor{red}{[todo and ref. the correct section in methods]} the probabilistic model is advantageous when only few data points are available. To show this effect in practice we trained mappings with and without the use of our method on randomly chosen subsets of the MNIST training data set, effectively simulating sparsity of available training data. Evaluation of the mappings was carried out on the full MNIST test set as in previous experiments. A line plot of the obtained scores corresponding to the chosen size of the subsets can be found in figure \ref{fig:downsampled}. Significant differences in the obtained scores are noted in favor of our method. From these results we can observe that both the trustworthiness and 1-NN scores remain higher even with extremely small data sets, indicating that the use of our method is able to provide better generalization even when few training data points are available.

\begin{figure}[!htb]
  \centering
  \includegraphics[width=\textwidth]{images/downsampled.png}
  \caption{1-NN and trustworthiness scores obtained for PTSNE and VPTSNE trained on small subsets of the MNIST data set. Error bars indicate the 95\% confidence intervals of the mean for each sparsity level, obtained from 20 repeated runs.}
  \label{fig:downsampled}
\end{figure}

\section{Robustness to Noisy Data}

As real world data is generally not perfectly clean, an important property for any machine learning method is its capability to handle data containing artifacts. Considering VAEs are inherently robust to corrupted data due to the regularization provided by the stochastic latent code (\textcolor{red}{[should be discussed in a previous section more thoroughly, connection with VAE and RPCA \citep{rpca_1, rpca_2} shown in \citep{hidden_talents_of_the_vae}]}) it is reasonable to assume that a training procedure for a low dimensional mapping taking advantage of this property would perform better on corrupted data than one that is subjected only to the raw data.

Here we aim to back up this assumption by running experiments on artificially corrupted MNIST data sets. As the corruption process in our experiments we consider \textit{masking noise} as in \cite{stacked_denoising_ae}, where a predetermined fraction of randomly chosen elements of a data point are set to 0. We test robustness by applying masking noise to 10\%, 20\%, 30\% and 40\% fractions of the training and test sets. The results of our experiments (cf. figure \ref{fig:corrupted_data}) show a clear advantage to using our method when dealing with noisy data as both evaluation metrics can be observed to decrease considerably for PTSNE as the training data is progressively corrupted, whereas VPTSNE maintains its performance throughout the experiments.

\noindent \textcolor{red}{[Note: Results look good, but could they be further improved by denoising variational autoencoders \citep{denoising_vae}?]}

\noindent \textcolor{red}{[Note: In scRNA-seq dropout events can be viewed as masking noise. The results here would indicate applying the method to scRNA-seq data could produce good results.]}

\noindent \textcolor{red}{[Note: Should other corruption processes be considered here or is masking noise sufficient?]}

\noindent \textcolor{red}{[Note: Could add scatter plot of the best scoring embeddings PTSNE vs VPTSNE to the supplement for e.g. the 30\% corrupted runs. The difference is quite striking.]}

\begin{figure}[!htb]
  \centering
  \includegraphics[width=\textwidth]{images/corrupted.png}
  \caption{ 1-NN and trustworthiness scores obtained for PTSNE and VPTSNE trained on MNIST data with different levels of masking noise applied. Error bars indicate the 95\% confidence intervals of the mean for each corruption level, obtained from 20 repeated runs.}
  \label{fig:corrupted_data}
\end{figure}

\textcolor{red}{ Again, I would want more points on the graphs. Noise level on x axis and score on y. }

\section{Obtaining Reconstructions from Hidden Layers}

\textcolor{red}{[Note: Much of this will probably be moved to the methods section.]}

Instead of training the embedding network on the final output of the observation model it is possible to instead use the outputs of a chosen hidden layer. If the chosen hidden layer has dimensions considerably smaller than that of the original, the benefits of this approach are twofold:

\begin{itemize}
\item We are performing a step of nonlinear dimensionality reduction that is not dependent on local distance metrics, i.e. a preprocessing step that is less susceptible to the \textit{curse of dimensionality}.
\item Given that the number of dimensions can be chosen to be considerably lower than in the original input space, computing the t-SNE loss also becomes proportionally cheaper.
\end{itemize}

To show the efficacy of this modification to the learning procedure we train an embedding using the latent code directly. As we choose the dimensionality of the latent code to be more than two orders of magnitude smaller than that of the original data's it is possible for us to use batches of far greater size to train the embedding network, without incurring additional computation cost over training with smaller batches on data of the original dimensionality. The latent dimensionality of the VAE was chosen to be $7$ and the batch size for training the embedding was set to $5000$, as in the original parametric t-SNE paper by \cite{parametric_tsne}. We have additionally included the embedding of data points obtained with the original implementation of parametric t-SNE for comparison in figure \ref{fig:hidden_layer}. Qualitatively and quantitatively we achieve an overall better embedding with less computation using our method.

When considering precomputed $P_{ij}$ matrices for each batch the gain in computational advantage does however diminish significantly, although the following caveats apply to the precomputation: Storing the precomputed matrices will require considerable space for large batch sizes and data sets. A way to circumvent the need for large storage space is to employ a KNN approximation when computing $P_{ij}$, i.e. only considering a small number of neighbors when computing the conditional probabilities for each pair of points, which will result in a sparse matrix. This however increases the error in the computed gradient and relies on an efficient KNN algorithm in practice. Additionally, fixing the batches in advance hinders the performance of SGD as the gradient estimates become biased. Moreover, relying on precomputed $P_{ij}$ matrices for fixed batches prevents the use of infinite sample generation from the VAE.

\begin{figure}[!htb]
  \centering
  \includegraphics[width=\textwidth]{images/vptsne_vs_original_ptsne_7.png}
  \caption{Embedding produced by training on a 7 dimensional latent representation of the MNIST data set contrasted with the original implementation of \cite{parametric_tsne} run on the original data. VPTSNE trustworthiness: 0.927, 1-NN: 0.910. PTSNE trustworthiness: 0.926, 1-NN 0.887.}
  \label{fig:hidden_layer}
\end{figure}

\textcolor{red}{[Note: The choice of 7 as the latent dimensionality right now seems unjustified. It was simply chosen to be the largest number for which "more than two orders of magnitude smaller" holds true. Should work that into the text.]}

\section{Inference with the Generative Model}

From the underlying generative model we can approximately compute the marginal $p(\mathbf{x})$ for each data point $\mathbf{x}$. An approximation can be retrieved by importance sampling \citep{stochastic_backpropagation}:

$$p(\mathbf{x}) \approx \frac{1}{N} \sum_{i=1}^{N} \frac{p(\mathbf{x} \vert \mathbf{z}^{(i)}) p(\mathbf{z}^{(i)})}{q_{\theta}(\mathbf{z}^{(i)} \vert \mathbf{x})},$$

where $\mathbf{z}^{(i)} \sim q_{\theta}(\mathbf{z} \vert \mathbf{x})$.

Using the obtained marginals as feedback we can inform the user of potentially anomalous points in the data set. This feedback can then further be used in downstream analysis. From a data visualization perspective points that do not fit the model well correspond to points that are likely to be mapped poorly, thus removing such outliers from the final mapping serves to highlight the natural clusters in the data more clearly.

\textcolor{red}{[Note: discuss the more accurate but computationally costly marginal estimation method: annealed importance sampling \citep{ais, ais_vae}?]}

\textcolor{red}{[Note: could discuss, experiment with modification to the algorithm: if p(x) (or even more simply just the reconstruction error) is high (above some threshold), pass the original data point and not the reconstruction to the mapping. This proved to work for the small subset experiment when limiting the latent dimension to $\leq 3$. Reason for this being that the VAE trained on only 600 samples poorly reconstructed the test set (basically just corrupting many of them).]}

\textcolor{red}{[Note: contents above should be moved to the methods section.]}

As an example we use the Cytometry B data set to train our combined model with two patients' data held out. The protein expression profile of Patient A's cells closely match that of other patients' in the training set, whereas Patient B's sample is from advanced cancer making it an outlier in relation to the data available during training.

To distinguish between outlier and inlier points in the mapping we obtain we set a hard threshold of $\log p(\mathbf{x}) < -150$ to separate between the two. In figure \ref{fig:cytometry_outliers} embeddings with both patient specific labeling and labeling obtained by thresholding are shown side by side. The high tumor purity sample of Patient B has almost completely been mapped to a small region corresponding to protein expression levels of cancer cells, whereas Patient A's mapping contains several distinct clusters of various cell types. Further, the labels obtained through thresholding indicate that the majority of cells in Patient B's sample are indeed considered as outliers by the model, in addition to a handful of visually poorly mapped points.

\begin{figure}[!htb]
  \centering
  \begin{tabular}{c}
    \includegraphics[width=\textwidth]{images/scored.png} \\
    \includegraphics[width=\textwidth]{images/scored_zoomed.png}
  \end{tabular}
  \caption{Detecting outliers in cytometry data. \textcolor{red}{[Rename patients to match text.]}}
  \label{fig:cytometry_outliers}
\end{figure}

\chapter{Conclusions}
\label{ch:conclusions}

