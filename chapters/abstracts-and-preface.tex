\thesisdescription{Master of Science Thesis}
\begin{abstract}

\textcolor{red}{TODO, incomplete}

Current generation biological measurement technologies enable quantifying cellular characteristics and processes at great resolution. [some concrete numbers?]

Dimensionality reduction is commonly employed in the analysis of such high dimensional molecular data, either as a preprocessing step for further downstream analysis or for purposes of exploratory data analysis by visualizing the data in very low dimensions.

However, these vast amounts of high dimensional data pose several challenges to dimensionality reduction methods in common use today.
\begin{itemize}
\item Linear methods like PCA are incapable of capturing the nonlinear, heteroscedastic nature of the data
\item The computational complexity of nonlinear methods such as t-SNE becomes an obstacle
\item In addition, the prevailing nonlinear methods are based on distances, rendering them prone to the curse of dimensionality
\end{itemize}

In this work a method for nonlinear dimensionality reduction is proposed which aims to address these issues. By building on top of a stochastic variant of autoencoder neural networks [... benefits stemming from the stochastic model ...]

The proposed method is compared to existing methods with favorable results in terms of its scalability to large data sets, robustness to sparse and corrupt data as well as its ability to combat the curse of dimensionality. Additionally, the practical application of the proposed method to two single-cell data sets is demonstrated.

\end{abstract}

\begin{otherlanguage}{finnish}

\title{Otsikoimaton}
\programme{Teknis-luonnontieteellinen diplomi-insinöörin tutkinto-ohjelma}
\thesisdescription{Diplomityö}
\major{Ohjelmistotiede}
\examiner{}
\keywords{}

\begin{abstract}

\textcolor{red}{TODO}

\end{abstract}
\end{otherlanguage}

\chapter*{Preface}

\textcolor{red}{TODO}

\vspace{2\baselineskip}

In Helsinki, Finland, on \today

\vspace{2\baselineskip}

Aleksi Hietanen

\tableofcontents
\listoffigures
\listoftables
\chapter*{List of Symbols and Abbreviations}

% This is not a "proper" table, so no table environment

% Suppressed left colsep; 20% - 1 x colsep; right colpsep; left colpadding; 80% - 1 x colpadding; suppressed right colpadding
\begin{tabular}[h]{@{} p{0.2\textwidth-\tabcolsep} p{0.8\textwidth-\tabcolsep} @{}}
AE        & autoencoder \\
PCA       & principal component analysis \\
GPU       & graphics processing unit \\
GPGPU     & general-purpose computing on graphics processing units \\
TPU       & tensor processing unit \\
MSE       & mean squared error \\
SGD       & stochastic gradient descent \\
VI        & variational inference \\
MCMC      & Markov chain Monte Carlo \\
RBM       & restricted Boltzmann machine \\
VAE       & variational autoencoder \\
GAN       & generative adversarial network \\
UMAP      & uniform manifold approximation and projection \\
SNE       & stochastic neighbor embedding \\
ReLU      & rectifier linear unit \\
t-SNE     & t-distributed stochastic neighbor embedding \\
k-NN      & k-nearest neighbors \\
RNA       & ribonucleic acid \\
scRNA-seq & single-cell RNA sequencing
\end{tabular}

\begin{tabular}[h]{@{} p{0.15\textwidth-\tabcolsep} p{0.85\textwidth-\tabcolsep} @{}}
$\mathcal{N}(\bmu, \bSigma)$  & multivariate Gaussian distribution with mean $\bmu$ and covariance $\bSigma$ \\
$p(x \vert y)$                & conditional probability of x given y \\
$\sim$                        & is distributed as \\
$\frac{\partial}{\partial x}$ & partial derivative w.r.t. $x$ \\
$\nabla f$                    & gradient of $f$ \\
$H(p)$                        & entropy of a random variable with distribution $p$ \\
$H(p,q)$                      & cross-entropy of distributions $p$ and $q$ \\
$\dkl{p}{q}$                  & Kullback-Leibler divergence from $q$ to $p$ \\
$\mathbb{E}_{p}[q]$           & Expectation of $q$ under distribution $p$ \\
$\mathcal{O}(f)$              & asymptotic upper bound of $f$ \\
\end{tabular}
